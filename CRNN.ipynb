{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Paper Referred Link:\n",
    "##### Attention based convolutional recurrent neural network for environmental sound classification (https://www.sciencedirect.com/science/article/pii/S0925231220313618)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.signal import stft\n",
    "from keras import layers, regularizers,optimizers,callbacks\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "input_dir='./Datasets/ESC-10/'\n",
    "model_path='./Models/CRNN.h5'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_file(file_path, sr=44100):\n",
    "    \"\"\"\n",
    "    Description: Load an audio file from the specified file path.\n",
    "    Arguments:\n",
    "        file_path (str): Path to the audio file.\n",
    "        sr (int): Desired sampling rate.\n",
    "    Returns:\n",
    "        audio (np.ndarray): Loaded audio data.\n",
    "    \"\"\"\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    return audio\n",
    "\n",
    "def extract_log_gammatone_spectrogram(signal, sample_rate):\n",
    "    \"\"\"\n",
    "    Decription: Extract the log-gammatone spectrogram from an audio signal.\n",
    "    Arguments:\n",
    "        signal (np.ndarray): Audio signal.\n",
    "        sample_rate (int): Sampling rate of the audio signal.\n",
    "    Returns:\n",
    "        frames (np.ndarray): Extracted log-gammatone spectrogram frames.\n",
    "        frames.shape[0] (int): Number of Labels to be added\n",
    "    \"\"\"\n",
    "    signal=librosa.util.normalize(signal)\n",
    "    epsilon = 1e-10  \n",
    "    window_size = int(0.023 * sample_rate)\n",
    "    hop_length = window_size // 2\n",
    "    _, _, stft_data = stft(signal, window='hamming', nperseg=window_size, noverlap=hop_length)\n",
    "    \n",
    "    energy_spectrogram = np.abs(stft_data)**2\n",
    "    \n",
    "    num_filters = 128\n",
    "    gammatone_spectrogram = gammatone_filter_bank(energy_spectrogram, sample_rate, num_filters)\n",
    "    log_spectrogram = np.log10(gammatone_spectrogram + epsilon)\n",
    "    frame_length = 128\n",
    "    frame_hop = int(frame_length * 0.5)\n",
    "    num_frames = (log_spectrogram.shape[1] - frame_length) // frame_hop + 1\n",
    "    frames = np.zeros((num_frames, frame_length,frame_length, 2))\n",
    "    for i in range(num_frames):\n",
    "        start = i * frame_hop\n",
    "        end = start + frame_length\n",
    "        frame = log_spectrogram[:, start:end]\n",
    "        \n",
    "        delta = np.diff(frame, axis=1)\n",
    "        delta = np.pad(delta, ((0, 0), (0, 1)), mode='constant')\n",
    "        frames[i, :, :] = np.dstack((frame, delta))\n",
    "    return frames, frames.shape[0]\n",
    "def gammatone_filter_bank(spectrogram, sample_rate, num_filters):\n",
    "    \"\"\"\n",
    "    Description: Apply a gammatone filter bank to a spectrogram.\n",
    "    Arguments:\n",
    "        spectrogram (np.ndarray): Input spectrogram.\n",
    "        sample_rate (int): Sampling rate of the audio signal.\n",
    "        num_filters (int): Number of filters in the filter bank.\n",
    "    Returns:\n",
    "        gammatone_spectrogram (np.ndarray): Gammatone-filtered spectrogram.\n",
    "    \"\"\"\n",
    "    num_bins = spectrogram.shape[0]\n",
    "    nyquist = sample_rate / 2\n",
    "    frequencies = np.linspace(0, nyquist, num_bins)\n",
    "    center_frequencies = erb_to_hertz(np.linspace(hertz_to_erb(frequencies[0]), hertz_to_erb(frequencies[-1]), num_filters))\n",
    "    \n",
    "    gammatone_spectrogram = np.zeros((num_filters, spectrogram.shape[1]))\n",
    "    \n",
    "    for i in range(num_filters):\n",
    "        filter_output = gammatone_filter(spectrogram, sample_rate, center_frequencies[i])\n",
    "        gammatone_spectrogram[i, :] = np.sum(np.abs(filter_output)**2, axis=0)\n",
    "    \n",
    "    return gammatone_spectrogram\n",
    "\n",
    "def gammatone_filter(signal, sample_rate, center_frequency):\n",
    "    \"\"\"\n",
    "    Description: Apply a gammatone filter to a signal.\n",
    "    Arguments:\n",
    "        signal (np.ndarray): Input signal.\n",
    "        sample_rate (int): Sampling rate of the audio signal.\n",
    "        center_frequency (float): Center frequency of the gammatone filter.\n",
    "    Returns:\n",
    "        filter_output (np.ndarray): Filtered signal.\n",
    "    \"\"\"\n",
    "    num_samples = signal.shape[1]\n",
    "    time = np.arange(num_samples) / sample_rate\n",
    "    \n",
    "    b, a = gammatone_coefficients(sample_rate, center_frequency)\n",
    "    filter_output = np.zeros_like(signal)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        t = time[i]\n",
    "        envelope = t**3 * np.exp(-2 * np.pi * 2.1 * t)\n",
    "        filter_output[:, i] = envelope * signal[:, i]\n",
    "    \n",
    "    return filter_output\n",
    "\n",
    "def gammatone_coefficients(sample_rate, center_frequency):\n",
    "    \"\"\"\n",
    "    Description: Compute the coefficients for a gammatone filter.\n",
    "    Arguments:\n",
    "        sample_rate (int): Sampling rate of the audio signal.\n",
    "        center_frequency (float): Center frequency of the gammatone filter.\n",
    "    Returns:\n",
    "        b (list): Numerator coefficients of the filter transfer function.\n",
    "        a (list): Denominator coefficients of the filter transfer function.\n",
    "    \"\"\"\n",
    "    t = 1 / sample_rate\n",
    "    bandwidth = 24.7 * (4.37 * center_frequency / 1000 + 1)\n",
    "    beta = 1.019 * 2 * np.pi * bandwidth / sample_rate\n",
    "    \n",
    "    b0 = t**2 * 2 * np.pi * center_frequency / sample_rate\n",
    "    b1 = 0\n",
    "    b2 = -b0\n",
    "    a0 = 1 + beta\n",
    "    a1 = -2 * np.cos(center_frequency * 2 * np.pi * t) / a0\n",
    "    a2 = (1 - beta) / a0\n",
    "\n",
    "    b = [b0, b1, b2]\n",
    "    a = [a0, a1, a2]\n",
    "\n",
    "    return b, a\n",
    "\n",
    "def hertz_to_erb(frequency):\n",
    "    \"\"\"\n",
    "    Description: Convert a frequency value from Hertz to ERB scale.\n",
    "    Arguments:\n",
    "        frequency (float): Frequency value in Hertz.\n",
    "    Returns:\n",
    "        erb (float): Frequency value in ERB scale.\n",
    "    \"\"\"\n",
    "    return 9.265 * np.log(1 + (frequency / 24.7) * 0.00437)\n",
    "\n",
    "def erb_to_hertz(erb):\n",
    "    \"\"\"\n",
    "    Description: Convert a frequency value from ERB scale to Hertz.\n",
    "    Arguments:\n",
    "        erb (float): Frequency value in ERB scale.\n",
    "    Returns:\n",
    "        frequency (float): Frequency value in Hertz.\n",
    "    \"\"\"\n",
    "    return 24.7 * (np.exp(erb / 9.265) - 1) / 0.00437\n",
    "\n",
    "def count_files():\n",
    "    \"\"\"\n",
    "    Description: Counts Number of Files in the dataset\n",
    "    Arguments:\n",
    "        None\n",
    "    Returns:\n",
    "        count (int): Number of files in the dataset\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for root_dir, cur_dir, files in os.walk(input_dir):\n",
    "        count += len(files)\n",
    "    return count\n",
    "\n",
    "def load_audio_files(directory_path, sr=44100):\n",
    "    \"\"\"\n",
    "    Description: Load audio files from a directory and extract log-gammatone spectrograms.\n",
    "    Arguments:\n",
    "        directory_path (str): Path to the directory containing audio files.\n",
    "        sr (int): Desired sampling rate.\n",
    "    Returns:\n",
    "        data (np.ndarray): Extracted log-gammatone spectrograms.\n",
    "        labels (np.ndarray): Corresponding labels for each spectrogram.\n",
    "    \"\"\"\n",
    "    data=np.empty((0,128,128,2))\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    label_idx = 0    \n",
    "    ctr=0\n",
    "    for label in os.listdir(directory_path):\n",
    "        path = os.path.join(directory_path, label)\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith('.ogg'):\n",
    "                label = os.path.basename(path)\n",
    "                if label not in label_map:\n",
    "                    label_map[label] = label_idx\n",
    "                    label_idx += 1\n",
    "                label_id = label_map[label]\n",
    "                file_path = os.path.join(path, file)\n",
    "                audio = load_audio_file(file_path, sr=sr)\n",
    "                log_gammatone_spectrogram, label_count = extract_log_gammatone_spectrogram(audio, sample_rate=sr)\n",
    "                data=np.concatenate([data,log_gammatone_spectrogram])\n",
    "                for i in range(label_count):\n",
    "                    labels.append(label_id)\n",
    "                ctr+=1\n",
    "    labels = np.asarray(labels)\n",
    "    return data, labels\n",
    "\n",
    "def preprocess_audio(audio_path, sr=44100):\n",
    "    \"\"\"\n",
    "    Description: Preprocess an audio file by loading it and extracting the log-gammatone spectrogram.\n",
    "    Arguments:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        sr (int): Desired sampling rate.\n",
    "    Returns:\n",
    "        spectrogram (np.ndarray): Extracted log-gammatone spectrogram.\n",
    "    \"\"\"\n",
    "    audio = load_audio_file(audio_path, sr=sr)\n",
    "    spectrogram = extract_log_gammatone_spectrogram(audio, sample_rate=sr)\n",
    "    return spectrogram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_attention_map(M):\n",
    "    \"\"\"\n",
    "    Description: Calculate the attention map from the given feature map.\n",
    "    Arguments:\n",
    "        M (tf.Tensor): Feature map tensor.\n",
    "    Returns:\n",
    "        attention_map (tf.Tensor): Calculated attention map.\n",
    "    \"\"\"\n",
    "    conv_output = tf.keras.layers.Conv2D(filters=1, kernel_size=(3, 3), padding='same')(M)\n",
    "    \n",
    "    avg_pool_output = tf.keras.layers.AveragePooling2D(pool_size=(conv_output.shape[1], 1))(conv_output)\n",
    "    \n",
    "    attention_map = tf.keras.activations.softmax(avg_pool_output, axis=1)\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "def apply_attention(M, attention_map):\n",
    "    \"\"\"\n",
    "    Description: Apply attention to the given feature map using the attention map.\n",
    "    Arguments:\n",
    "        M (tf.Tensor): Feature map tensor.\n",
    "        attention_map (tf.Tensor): Attention map tensor.\n",
    "    Returns:\n",
    "        M0 (tf.Tensor): Feature map with applied attention.\n",
    "    \"\"\"\n",
    "    M0 = tf.multiply(M, attention_map)\n",
    "    \n",
    "    return M0\n",
    "\n",
    "def calculate_attention_weights(inputs):\n",
    "    \"\"\"\n",
    "    Description: Calculate the attention weights from the given inputs.\n",
    "    Arguments:\n",
    "        inputs (tf.Tensor): Input tensor.\n",
    "    Returns:\n",
    "        weights (tf.Tensor): Calculated attention weights.\n",
    "    \"\"\"\n",
    "    hidden = layers.Dense(units=256, activation='tanh')(inputs)\n",
    "    \n",
    "    weights = tf.exp(hidden) / tf.reduce_sum(tf.exp(hidden), axis=1, keepdims=True)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def compute_feature_vector(ht, attention_weights):\n",
    "    \"\"\"\n",
    "    Description: Compute the weighted sum of the frame-level RNN features.\n",
    "    Arguments:\n",
    "        ht (tf.Tensor): RNN features tensor.\n",
    "        attention_weights (tf.Tensor): Attention weights tensor.\n",
    "    Returns:\n",
    "        feature_vector (tf.Tensor): Computed feature vector.\n",
    "    \"\"\"\n",
    "    feature_vector = tf.reduce_sum(tf.multiply(ht, attention_weights), axis=1)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "def build_crnn_model_with_attention(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Description: Build a CRNN model with attention mechanism.\n",
    "    Arguments:\n",
    "        input_shape (tuple): Shape of the input tensor.\n",
    "        num_classes (int): Number of output classes.\n",
    "    Returns:\n",
    "        model (tf.keras.Model): Built CRNN model with attention mechanism.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    cnn = layers.Conv2D(32,(3,5),(1,1), activation='leaky_relu', padding='same',kernel_regularizer=regularizers.L2(0.0001))(inputs)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.Conv2D(32,(3,5),(1,1), activation='leaky_relu', padding='same',kernel_regularizer=regularizers.L2(0.0001))(inputs)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.MaxPooling2D(strides=(4, 3))(cnn)\n",
    "    cnn = layers.Conv2D(64, (3, 1),(1,1), activation='leaky_relu', padding='same',kernel_regularizer=regularizers.L2(0.0001))(cnn)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.Conv2D(64, (3, 1),(1,1), activation='leaky_relu', padding='same',kernel_regularizer=regularizers.L2(0.0001))(cnn)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.MaxPooling2D(strides=(4, 1))(cnn)\n",
    "    cnn = layers.Conv2D(128, (1, 5),(1,1), activation='leaky_relu', padding='same',kernel_regularizer=regularizers.L2(0.0001))(cnn)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.Conv2D(128, (1, 5),(1,1), activation='leaky_relu', padding='same',kernel_regularizer=regularizers.L2(0.0001))(cnn)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.MaxPooling2D(strides=(1,3))(cnn)\n",
    "    cnn = layers.Conv2D(256, (3, 3),(1,1), activation='leaky_relu', padding='same',kernel_regularizer=regularizers.L2(0.0001))(cnn)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.Conv2D(256, (3, 3),(1,1), activation='leaky_relu', padding='same',kernel_regularizer=regularizers.L2(0.0001))(cnn)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.MaxPooling2D(strides=(2, 2))(cnn)\n",
    "    \n",
    "    cnn_attmap = calculate_attention_map(cnn)\n",
    "    cnn_att=apply_attention(cnn,cnn_attmap)\n",
    "\n",
    "    rnn = layers.Reshape((-1, 256))(cnn_att)\n",
    "    rnn = layers.Bidirectional(layers.GRU(256,activation=\"tanh\", return_sequences=True,kernel_regularizer=regularizers.L2(0.0001)))(rnn)\n",
    "    rnn = layers.Dropout(0.5)(rnn)\n",
    "    rnn = layers.Bidirectional(layers.GRU(256,activation=\"tanh\", return_sequences=True,kernel_regularizer=regularizers.L2(0.0001)))(rnn)\n",
    "    rnn = layers.Dropout(0.5)(rnn)\n",
    "    rnn = layers.Reshape((-1, 256))(rnn)\n",
    "    \n",
    "    att_weights = calculate_attention_weights(rnn)\n",
    "    attended_features = compute_feature_vector(rnn, att_weights)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(attended_features)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Predict Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_train():\n",
    "    \"\"\"\n",
    "    Description: Pipeline for training a CRNN model on the ESC-10 dataset.\n",
    "    Arguments:\n",
    "        None\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    batch_size = 16\n",
    "    epochs = 50\n",
    "    init_lr = 0.01\n",
    "    lr_decay = 0.1\n",
    "    lr_drop = 16\n",
    "\n",
    "    X, y = load_audio_files(input_dir, sr=44100)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "    input_shape = X_train[0].shape\n",
    "    model = build_crnn_model_with_attention(input_shape, num_classes)\n",
    "    \n",
    "    weights = model.get_weights()\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] = np.random.normal(loc=0.0, scale=0.05, size=weights[i].shape)\n",
    "    model.set_weights(weights)\n",
    "    \n",
    "    opt = optimizers.SGD(learning_rate=init_lr, decay=lr_decay, momentum=0.9, nesterov=True,clipnorm=1.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        lr = init_lr * (lr_decay ** np.floor((1 + epoch) / lr_drop))\n",
    "        return lr\n",
    "\n",
    "    lr_callback = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[lr_callback,checkpoint])\n",
    "    \n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('Test loss:', loss)\n",
    "    print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_predict(filepath):\n",
    "    \"\"\"\n",
    "    Description: Pipeline for predicting the class of an audio file using a pre-trained CRNN model.\n",
    "    Arguments:\n",
    "        filepath (str): Path to the audio file.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model = load_model(model_path)\n",
    "    spectrogram = preprocess_audio(filepath)\n",
    "    predictions = model.predict(spectrogram[0])\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 1 -r 2 pipeline_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_predict('./Datasets/ESC-10/001 - Dog bark/1-30226-A.ogg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
